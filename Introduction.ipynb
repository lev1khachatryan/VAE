{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video: Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "[Paper: Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)\n",
    "\n",
    "[Paper-Tutorial: An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf)\n",
    "\n",
    "[Paper-Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function](https://arxiv.org/pdf/1907.08956.pdf)\n",
    "\n",
    "[Post: Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "[Video: Bayesian Inference and VAEs](https://www.youtube.com/watch?v=uaaqyVS9-rM)\n",
    "\n",
    "[Paper: PixelRNN, PixelCNN](https://arxiv.org/pdf/1601.06759.pdf)\n",
    "\n",
    "[Paper: Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/pdf/1606.05328.pdf)\n",
    "\n",
    "[Paper: Variational Lossy Autoencoder (VLAE)](https://arxiv.org/pdf/1611.02731.pdf)\n",
    "\n",
    "[Paper: Neural Discrete Representation Learning (VQVAE)](https://arxiv.org/pdf/1711.00937.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>1. Types of Generative Models</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can distinguish two main types of generative models:\n",
    "\n",
    "1. Likelihood Based Models - include ***VAE***s, ***Flow Based*** and ***Autoregressive*** models; and\n",
    "2. Implicit Generative Models - such as ***GAN***s.\n",
    "\n",
    "### Short Introduction\n",
    "\n",
    "* GANs pose the training process as a game between two separate networks: a generator network and a second discriminative network that tries to classify samples as either coming from the true distribution $p(x)$ or the model distribution $\\hat{p}(x)$. Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away, until at the end (in theory) the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference. Larger scale GAN models can now generate high-quality and high resolution images. However, it is well known that *samples from these models do not fully capture the diversity of the true distribution*. Furthermore, GANs are challenging to evaluate, and a satisfactory generalization measure on a test set to assess overfitting does not yet exist. For model comparison and selection, researchers have used image samples or proxy measures of image quality such as ***Inception Score*** (IS) and ***Fréchet Inception Distance*** (FID).\n",
    "\n",
    "* In contrast, likelihood based methods optimize negative log-likelihood (NLL) of the training data. This objective allows model-comparison and measuring generalization to unseen data. Additionally, *since the probability that the model assigns to all examples in the training set is maximized, likelihood based models, in principle, cover all modes of the data, and do not suffer from the problems of mode collapse and lack of diversity seen in GANs*.\n",
    "\n",
    "In this project, we consider likelihood based models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>2. Probability Theory Summary</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Probability Density Function $p(X)$*** - is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be equal to that sample. While there is only one true density function $p_{data}$ that is assumed to have generated the observable dataset, there are infinitely many density functions $p_{model}$ that we can use to estimate $p_{data}$. In order to structure our approach to finding a suitable $p_{model}(X)$ we can use a technique known as parametric modeling.\n",
    "\n",
    "* ***Parametric Model $p_{\\theta}(X)$*** - is a family of density functions, that can be described using a finite number of parameters, $\\theta$.\n",
    "\n",
    "* ***Likelihood Function $L(\\theta | X)$*** - of a parameter set $\\theta$, is a function that measures the plausibility of $\\theta$, given some observed point $X$. So the likelihood function $L(\\theta | X) = p_{\\theta}(X)$. The focus of parametric modeling should be to find the optimal value $\\hat{\\theta}$ of the parameter set that maximizes the likelihood of observing the dataset $X$.\n",
    "\n",
    "* ***Maximum Likelihood Estimation*** - is the technique that allows us to estimate $\\hat{\\theta}$, the set of parameters $\\theta$ of a density function $p_{\\theta}(X)$, that are most likely to explain some observed data $X$. More formally $\\hat{\\theta} = argmax_{\\theta} L(\\theta | X)$\n",
    "\n",
    "* ***Variational Inference*** - is a technique to approximate complex distributions. The idea is to set a parametrised family of distribution (for example the family of Gaussians) and to look for the best approximation of our target distribution among this family. The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>3. Intro to VAEs</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper-tutorial ***Deriving the Standard Variational Autoencoder (VAE) Loss Function***, the authors have clearly explained the Objective of VAEs.\n",
    "\n",
    "It turns out, that\n",
    "\n",
    "<center>$log p(x_i) \\geq -D_{KL}(q_{\\theta}(z|x_i) || p(z)) + E_{q_{\\theta}(z | x_i)}(log (p_{\\phi}(x_i | z)))$</center>\n",
    "\n",
    "The right hand side of the above equation is the Evidence Lower Bound (ELBO) also known as the variational lower bound. It is so termed because it bounds the likelihood of the data which is the term we seek to maximize. Therefore ***maximizing the ELBO maximizes the log probability of our data by proxy***. This is the core idea of variational inference, since maximization of the log probability directly is typically computationally intractable. The Kullback-Leibler term in the ELBO is a regularizer because it is a constraint on the form of the approximate posterior. The second term is called a reconstruction term because it is a measure of the likelihood of the reconstructed data output at the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>4. Autoregressive Models - PixelRNN, PixelCNN</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chain rule to decompose likelihood of an image x into product of 1D distributions:\n",
    "\n",
    "<center>$p(x)=\\prod_{i=1}^{n^2}p(x_i| x_1, x_2, ..., x_{i-1})$</center>\n",
    "\n",
    "If images are RGB, then output of PixelCNN has shape of (BS, H, W, 3, 256). In contrast PixelRNN based on ***Diagonal BiLSTM***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>5. Variational Lossy Autoencoder (VLAE)</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here VAE is combined with autoregressive models such as RNN, MADE and PixelRNN/CNN. However, previous attempts have found it hard to benefit from VAE when using an expressive decoding distribution $p(x|z)$.In most cases when an RNN autoregressive decoding distribution is used, the latent code $z$ is completely ignored and the model regresses to be a standard unconditional RNN autoregressive distribution that doesn’t depend on the latent code. This phenomenon is commonly attributed to ***Posterior Collapse*** in the literature, because early in the training the approximate posterior $q(z|x)$ carries little information about datapoint x and hence it’s easy for the model to just set the *approximate posterior to be the prior* to avoid paying any regularization cost $D_{KL}(q(z|x)||p(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>6. VQ-VAE</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior and prior distributions are categorical, and the samples drawn from these distributions index an embedding table. These embeddings are then used as input into the decoder network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
