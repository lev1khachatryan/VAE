{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Gated Pixel CNN</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The original ***Pixel Recurrent Neural Networks*** paper](https://arxiv.org/abs/1601.06759)\n",
    "\n",
    "[The original ***Conditional Image Generation with PixelCNN Decoders*** paper](https://arxiv.org/abs/1606.05328)\n",
    "\n",
    "[PixelCNN](http://sergeiturukin.com/2017/02/22/pixelcnn.html)\n",
    "\n",
    "[Gated PixelCNN](http://sergeiturukin.com/2017/02/24/gated-pixelcnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import chainer.links as L\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.misc\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind Spot of PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What information layer ***L*** gets from ***L-2*** layer? We’re particularly interested in rightmost pixels as they extend our receptive field. What we see is that due to masking they won’t have access to some pixels layer ***L*** interested in. If we continue our logic it’s clear that layer ***L does not see*** some pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[[[ 0.  1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.  9.]\n",
      "   [10. 11. 12. 13. 14.]\n",
      "   [15. 16. 17. 18. 19.]\n",
      "   [20. 21. 22. 23. 24.]]]]\n",
      "kernel:  [[1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "input modified:  [[[[   0.    1.    2.    3.    4.]\n",
      "   [   5.    6.    7.    8. 1000.]\n",
      "   [  10.   11.   12.   13.   14.]\n",
      "   [  15.   16.   17.   18.   19.]\n",
      "   [  20.   21.   22.   23.   24.]]]]\n",
      "output:  [[[[64.]]]]\n",
      "input initial:  [[[[ 0.  1.  2.  3.  4.]\n",
      "   [ 5.  6.  7.  8.  9.]\n",
      "   [10. 11. 12. 13. 14.]\n",
      "   [15. 16. 17. 18. 19.]\n",
      "   [20. 21. 22. 23. 24.]]]]\n",
      "output:  [[[[64.]]]]\n"
     ]
    }
   ],
   "source": [
    "# create 5x5 input\n",
    "# chainer requires input to have shape [BATCH, CHANNELS, HEIGHT, WIDTH]\n",
    "input = np.arange(25).reshape([1,1,5,5]).astype('f')\n",
    "print('input: ', input)\n",
    "# array([[[[    0.,     1.,     2.,     3.,     4.],\n",
    "#          [    5.,     6.,     7.,     8.,     9.],\n",
    "#          [   10.,    11.,    12.,    13.,    14.],\n",
    "#          [   15.,    16.,    17.,    18.,    19.],\n",
    "#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)\n",
    "\n",
    "# create kernel of ones so it just sums all values within\n",
    "# use one for simplicity: easy to check\n",
    "kernel = np.ones([3, 3])\n",
    "# turn to proper type 'A' mask\n",
    "kernel[2:, :] = 0.0\n",
    "kernel[1, 1:] = 0.0\n",
    "print('kernel: ', kernel)\n",
    "# array([[ 1.,  1.,  1.],\n",
    "#        [ 1.,  0.,  0.],\n",
    "#        [ 0.,  0.,  0.]])\n",
    "\n",
    "# create two convolution layers with total receptive field size 5x5\n",
    "# so out input is exact fit\n",
    "\n",
    "l1 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)\n",
    "l2 = L.Convolution2D(1, 1, ksize=3, initialW=kernel)\n",
    "# c1 = K.layers.Conv2D(filters=1, kernel_size=3, padding='SAME', strides=(1, 1), kernel_initializer=lambda x: kernel)\n",
    "# c2 = K.layers.Conv2D(filters=1, kernel_size=3, padding='SAME', strides=(1, 1), kernel_initializer=lambda x: kernel)\n",
    "\n",
    "# here is the trick: pixel at [1, 4] position will be inside blind spot\n",
    "# if we perform convolution its value won't be included in final sum\n",
    "# so let's increase its value so it would be easy to check\n",
    "input[:, :, 1, 4] = 1000\n",
    "print('input modified: ', input)\n",
    "# array([[[[    0.,     1.,     2.,     3.,     4.],\n",
    "#          [    5.,     6.,     7.,     8.,  1000.],\n",
    "#          [   10.,    11.,    12.,    13.,    14.],\n",
    "#          [   15.,    16.,    17.,    18.,    19.],\n",
    "#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)\n",
    "\n",
    "output = l2(l1(input)).data\n",
    "print('output: ', output)\n",
    "# array([[[[ 64.]]]], dtype=float32)\n",
    "# Viola! Sum is lesser that 1000 which means pixel at [1, 4] wasn't seen!\n",
    "\n",
    "# Otherwise, let's return it value back\n",
    "input[:, :, 1, 4] = 9\n",
    "print('input initial: ', input)\n",
    "# array([[[[    0.,     1.,     2.,     3.,     4.],\n",
    "#          [    5.,     6.,     7.,     8.,     9.],\n",
    "#          [   10.,    11.,    12.,    13.,    14.],\n",
    "#          [   15.,    16.,    17.,    18.,    19.],\n",
    "#          [   20.,    21.,    22.,    23.,    24.]]]], dtype=float32)\n",
    "\n",
    "# perform computation again..\n",
    "output = l2(l1(input)).data\n",
    "print('output: ', output)\n",
    "# array([[[[ 64.]]]], dtype=float32)\n",
    "# Another evidence: no matter what value we assign to it final sum doesn't change\n",
    "# That proves it's within blind spot and we can't access information at it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(shape, name, horizontal, mask_mode='noblind', mask=None):\n",
    "    weights_initializer = tf.contrib.layers.xavier_initializer()\n",
    "    W = tf.get_variable(name, shape, tf.float32, weights_initializer)\n",
    "\n",
    "    '''\n",
    "        Use of masking to hide subsequent pixel values \n",
    "    '''\n",
    "    if mask:\n",
    "        filter_mid_y = shape[0]//2\n",
    "        filter_mid_x = shape[1]//2\n",
    "        mask_filter = np.ones(shape, dtype=np.float32)\n",
    "        if mask_mode == 'noblind':\n",
    "            if horizontal:\n",
    "                # All rows after center must be zero\n",
    "                mask_filter[filter_mid_y+1:, :, :, :] = 0.0\n",
    "                # All columns after center in center row must be zero\n",
    "                mask_filter[filter_mid_y, filter_mid_x+1:, :, :] = 0.0\n",
    "            else:\n",
    "                if mask == 'a':\n",
    "                    # In the first layer, can ONLY access pixels above it\n",
    "                    mask_filter[filter_mid_y:, :, :, :] = 0.0\n",
    "                else:\n",
    "                    # In the second layer, can access pixels above or even with it.\n",
    "                    # Reason being that the pixels to the right or left of the current pixel\n",
    "                    #  only have a receptive field of the layer above the current layer and up.\n",
    "                    mask_filter[filter_mid_y+1:, :, :, :] = 0.0\n",
    "\n",
    "            if mask == 'a':\n",
    "                # Center must be zero in first layer\n",
    "                mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.0\n",
    "        else:\n",
    "            mask_filter[filter_mid_y, filter_mid_x+1:, :, :] = 0.\n",
    "            mask_filter[filter_mid_y+1:, :, :, :] = 0.\n",
    "\n",
    "            if mask == 'a':\n",
    "                mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.\n",
    "                \n",
    "        W *= mask_filter \n",
    "    return W\n",
    "\n",
    "def get_bias(shape, name):\n",
    "    return tf.get_variable(name, shape, tf.float32, tf.zeros_initializer)\n",
    "\n",
    "def conv_op(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "class GatedCNN():\n",
    "    def __init__(self, W_shape, fan_in, horizontal, gated=True, payload=None, mask=None, activation=True, conditional=None, conditional_image=None):\n",
    "        self.fan_in = fan_in\n",
    "        in_dim = self.fan_in.get_shape()[-1]\n",
    "        self.W_shape = [W_shape[0], W_shape[1], in_dim, W_shape[2]]  \n",
    "        self.b_shape = W_shape[2]\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.payload = payload\n",
    "        self.mask = mask\n",
    "        self.activation = activation\n",
    "        self.conditional = conditional\n",
    "        self.conditional_image = conditional_image\n",
    "        self.horizontal = horizontal\n",
    "        \n",
    "        if gated:\n",
    "            self.gated_conv()\n",
    "        else:\n",
    "            self.simple_conv()\n",
    "\n",
    "    def gated_conv(self):\n",
    "        W_f = get_weights(self.W_shape, \"v_W\", self.horizontal, mask=self.mask)\n",
    "        W_g = get_weights(self.W_shape, \"h_W\", self.horizontal, mask=self.mask)\n",
    "\n",
    "        b_f_total = get_bias(self.b_shape, \"v_b\")\n",
    "        b_g_total = get_bias(self.b_shape, \"h_b\")\n",
    "        if self.conditional is not None:\n",
    "            h_shape = int(self.conditional.get_shape()[1])\n",
    "            V_f = get_weights([h_shape, self.W_shape[3]], \"v_V\", self.horizontal)\n",
    "            b_f = tf.matmul(self.conditional, V_f)\n",
    "            V_g = get_weights([h_shape, self.W_shape[3]], \"h_V\", self.horizontal)\n",
    "            b_g = tf.matmul(self.conditional, V_g)\n",
    "\n",
    "            b_f_shape = tf.shape(b_f)\n",
    "            b_f = tf.reshape(b_f, (b_f_shape[0], 1, 1, b_f_shape[1]))\n",
    "            b_g_shape = tf.shape(b_g)\n",
    "            b_g = tf.reshape(b_g, (b_g_shape[0], 1, 1, b_g_shape[1]))\n",
    "\n",
    "            b_f_total = b_f_total + b_f\n",
    "            b_g_total = b_g_total + b_g\n",
    "        if self.conditional_image is not None:\n",
    "            b_f_total = b_f_total + tf.layers.conv2d(self.conditional_image, self.in_dim, 1, use_bias=False, name=\"ci_f\")\n",
    "            b_g_total = b_g_total + tf.layers.conv2d(self.conditional_image, self.in_dim, 1, use_bias=False, name=\"ci_g\")\n",
    "\n",
    "        conv_f = conv_op(self.fan_in, W_f)\n",
    "        conv_g = conv_op(self.fan_in, W_g)\n",
    "       \n",
    "        if self.payload is not None:\n",
    "            conv_f += self.payload\n",
    "            conv_g += self.payload\n",
    "\n",
    "        self.fan_out = tf.multiply(tf.tanh(conv_f + b_f_total), tf.sigmoid(conv_g + b_g_total))\n",
    "\n",
    "    def simple_conv(self):\n",
    "        W = get_weights(self.W_shape, \"W\", self.horizontal, mask_mode=\"standard\", mask=self.mask)\n",
    "        b = get_bias(self.b_shape, \"b\")\n",
    "        conv = conv_op(self.fan_in, W)\n",
    "        if self.activation: \n",
    "            self.fan_out = tf.nn.relu(tf.add(conv, b))\n",
    "        else:\n",
    "            self.fan_out = tf.add(conv, b)\n",
    "\n",
    "    def output(self):\n",
    "        return self.fan_out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(object):\n",
    "    def __init__(self, X, conf, full_horizontal=True, h=None):\n",
    "        self.X = X\n",
    "        if conf.data == \"mnist\":\n",
    "            self.X_norm = X\n",
    "        else:\n",
    "            '''\n",
    "                Image normalization for CIFAR-10 was supposed to be done here\n",
    "            '''\n",
    "            self.X_norm = X\n",
    "        v_stack_in, h_stack_in = self.X_norm, self.X_norm\n",
    "\n",
    "        if conf.conditional is True:\n",
    "            if h is not None:\n",
    "                self.h = h\n",
    "            else:\n",
    "                self.h = tf.placeholder(tf.float32, shape=[None, conf.num_classes]) \n",
    "        else:\n",
    "            self.h = None\n",
    "\n",
    "        for i in range(conf.layers):\n",
    "            filter_size = 3 if i > 0 else 7\n",
    "            mask = 'b' if i > 0 else 'a'\n",
    "            residual = True if i > 0 else False\n",
    "            i = str(i)\n",
    "            with tf.variable_scope(\"v_stack\"+i):\n",
    "                v_stack = GatedCNN([filter_size, filter_size, conf.f_map], v_stack_in, False, mask=mask, conditional=self.h).output()\n",
    "                v_stack_in = v_stack\n",
    "\n",
    "            with tf.variable_scope(\"v_stack_1\"+i):\n",
    "                v_stack_1 = GatedCNN([1, 1, conf.f_map], v_stack_in, False, gated=False, mask=None).output()\n",
    "\n",
    "            with tf.variable_scope(\"h_stack\"+i):\n",
    "                h_stack = GatedCNN([filter_size if full_horizontal else 1, filter_size, conf.f_map], h_stack_in, True, payload=v_stack_1, mask=mask, conditional=self.h).output()\n",
    "\n",
    "            with tf.variable_scope(\"h_stack_1\"+i):\n",
    "                h_stack_1 = GatedCNN([1, 1, conf.f_map], h_stack, True, gated=False, mask=None).output()\n",
    "                if residual:\n",
    "                    h_stack_1 += h_stack_in # Residual connection\n",
    "                h_stack_in = h_stack_1\n",
    "\n",
    "        with tf.variable_scope(\"fc_1\"):\n",
    "            fc1 = GatedCNN([1, 1, conf.f_map], h_stack_in, True, gated=False, mask='b').output()\n",
    "\n",
    "        if conf.data == \"mnist\":\n",
    "            with tf.variable_scope(\"fc_2\"):\n",
    "                self.fc2 = GatedCNN([1, 1, 1], fc1, True, gated=False, mask='b', activation=False).output()\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fc2, labels=self.X))\n",
    "            self.pred = tf.nn.sigmoid(self.fc2)\n",
    "        else:\n",
    "            color_dim = 256\n",
    "            with tf.variable_scope(\"fc_2\"):\n",
    "                self.fc2 = GatedCNN([1, 1, conf.channel * color_dim], fc1, True, gated=False, mask='b', activation=False).output()\n",
    "                self.fc2 = tf.reshape(self.fc2, (-1, color_dim))\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.fc2, tf.cast(tf.reshape(self.X, [-1]), dtype=tf.int32)))\n",
    "\n",
    "            '''\n",
    "                Since this code was not run on CIFAR-10, I'm not sure which \n",
    "                would be a suitable way to generate 3-channel images. Below are\n",
    "                the 2 methods which may be used, with the first one (self.pred)\n",
    "                being more likely.\n",
    "            '''\n",
    "            self.pred_sampling = tf.reshape(tf.multinomial(tf.nn.softmax(self.fc2), num_samples=1, seed=100), tf.shape(self.X))\n",
    "            self.pred = tf.reshape(tf.argmax(tf.nn.softmax(self.fc2), dimension=tf.rank(self.fc2) - 1), tf.shape(self.X))\n",
    "\n",
    "\n",
    "class ConvolutionalEncoder(object):\n",
    "    def __init__(self, X, conf):\n",
    "        '''\n",
    "            This is the 6-layer architecture for Convolutional Autoencoder\n",
    "            mentioned in the original paper: \n",
    "            Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction\n",
    "            Note that only the encoder part is implemented as PixelCNN is taken\n",
    "            as the decoder.\n",
    "        '''\n",
    "\n",
    "        W_conv1 = get_weights([5, 5, conf.channel, 100], \"W_conv1\")\n",
    "        b_conv1 = get_bias([100], \"b_conv1\")\n",
    "        conv1 = tf.nn.relu(conv_op(X, W_conv1) + b_conv1)\n",
    "        pool1 = max_pool_2x2(conv1)\n",
    "\n",
    "        W_conv2 = get_weights([5, 5, 100, 150], \"W_conv2\")\n",
    "        b_conv2 = get_bias([150], \"b_conv2\")\n",
    "        conv2 = tf.nn.relu(conv_op(pool1, W_conv2) + b_conv2)\n",
    "        pool2 = max_pool_2x2(conv2)\n",
    "\n",
    "        W_conv3 = get_weights([3, 3, 150, 200], \"W_conv3\")\n",
    "        b_conv3 = get_bias([200], \"b_conv3\")\n",
    "        conv3 = tf.nn.relu(conv_op(pool2, W_conv3) + b_conv3)\n",
    "        conv3_reshape = tf.reshape(conv3, (-1, 7*7*200))\n",
    "\n",
    "        W_fc = get_weights([7*7*200, 10], \"W_fc\")\n",
    "        b_fc = get_bias([10], \"b_fc\")\n",
    "        self.pred = tf.nn.softmax(tf.add(tf.matmul(conv3_reshape, W_fc), b_fc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(images):\n",
    "    return (np.random.uniform(size=images.shape) < images).astype(np.float32)\n",
    "\n",
    "def generate_samples(sess, X, h, pred, conf, suff):\n",
    "    print(\"Generating Sample Images...\")\n",
    "    n_row, n_col = 10,10\n",
    "    samples = np.zeros((n_row*n_col, conf.img_height, conf.img_width, conf.channel), dtype=np.float32)\n",
    "    # TODO make it generic\n",
    "    labels = one_hot(np.array([0,1,2,3,4,5,6,7,8,9]*10), conf.num_classes)\n",
    "\n",
    "    for i in range(conf.img_height):\n",
    "        for j in range(conf.img_width):\n",
    "            for k in range(conf.channel):\n",
    "                data_dict = {X:samples}\n",
    "                if conf.conditional is True:\n",
    "                    data_dict[h] = labels\n",
    "                next_sample = sess.run(pred, feed_dict=data_dict)\n",
    "                if conf.data == \"mnist\":\n",
    "                    next_sample = binarize(next_sample)\n",
    "                samples[:, i, j, k] = next_sample[:, i, j, k]\n",
    "\n",
    "    save_images(samples, n_row, n_col, conf, suff)\n",
    "\n",
    "\n",
    "def generate_ae(sess, encoder_X, decoder_X, y, data, conf, suff=''):\n",
    "    print(\"Generating Sample Images...\")\n",
    "    n_row, n_col = 10,10\n",
    "    samples = np.zeros((n_row*n_col, conf.img_height, conf.img_width, conf.channel), dtype=np.float32)\n",
    "    if conf.data == 'mnist':\n",
    "        labels = binarize(data.train.next_batch(n_row*n_col)[0].reshape(n_row*n_col, conf.img_height, conf.img_width, conf.channel))\n",
    "    else:\n",
    "        labels = get_batch(data, 0, n_row*n_col) \n",
    "\n",
    "    for i in range(conf.img_height):\n",
    "        for j in range(conf.img_width):\n",
    "            for k in range(conf.channel):\n",
    "                next_sample = sess.run(y, {encoder_X: labels, decoder_X: samples})\n",
    "                if conf.data == 'mnist':\n",
    "                    next_sample = binarize(next_sample)\n",
    "                samples[:, i, j, k] = next_sample[:, i, j, k]\n",
    "\n",
    "    save_images(samples, n_row, n_col, conf, suff)\n",
    "\n",
    "\n",
    "def save_images(samples, n_row, n_col, conf, suff):\n",
    "    images = samples \n",
    "    if conf.data == \"mnist\":\n",
    "        images = images.reshape((n_row, n_col, conf.img_height, conf.img_width))\n",
    "        images = images.transpose(1, 2, 0, 3)\n",
    "        images = images.reshape((conf.img_height * n_row, conf.img_width * n_col))\n",
    "    else:\n",
    "        images = images.reshape((n_row, n_col, conf.img_height, conf.img_width, conf.channel))\n",
    "        images = images.transpose(1, 2, 0, 3, 4)\n",
    "        images = images.reshape((conf.img_height * n_row, conf.img_width * n_col, conf.channel))\n",
    "\n",
    "    filename = datetime.now().strftime('%Y_%m_%d_%H_%M')+suff+\".jpg\"\n",
    "    scipy.misc.toimage(images, cmin=0.0, cmax=1.0).save(os.path.join(conf.samples_path, filename))\n",
    "\n",
    "\n",
    "def get_batch(data, pointer, batch_size):\n",
    "    if (batch_size + 1) * pointer >= data.shape[0]:\n",
    "        pointer = 0\n",
    "    batch = data[batch_size * pointer : batch_size * (pointer + 1)]\n",
    "    pointer += 1\n",
    "    return [batch, pointer]\n",
    "\n",
    "\n",
    "def one_hot(batch_y, num_classes):\n",
    "    y_ = np.zeros((batch_y.shape[0], num_classes))\n",
    "    y_[np.arange(batch_y.shape[0]), batch_y] = 1\n",
    "    return y_\n",
    "\n",
    "\n",
    "def makepaths(conf):\n",
    "    ckpt_full_path = os.path.join(conf.ckpt_path, \"data=%s_bs=%d_layers=%d_fmap=%d\"%(conf.data, conf.batch_size, conf.layers, conf.f_map))\n",
    "    if not os.path.exists(ckpt_full_path):\n",
    "        os.makedirs(ckpt_full_path)\n",
    "    conf.ckpt_file = os.path.join(ckpt_full_path, \"model.ckpt\")\n",
    "\n",
    "    conf.samples_path = os.path.join(conf.samples_path, \"epoch=%d_bs=%d_layers=%d_fmap=%d\"%(conf.epochs, conf.batch_size, conf.layers, conf.f_map))\n",
    "    if not os.path.exists(conf.samples_path):\n",
    "        os.makedirs(conf.samples_path)\n",
    "\n",
    "    if tf.gfile.Exists(conf.summary_path):\n",
    "        tf.gfile.DeleteRecursively(conf.summary_path)\n",
    "    tf.gfile.MakeDirs(conf.summary_path)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(conf, data):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, conf.img_height, conf.img_width, conf.channel])\n",
    "    model = PixelCNN(X, conf)\n",
    "\n",
    "    trainer = tf.train.RMSPropOptimizer(1e-3)\n",
    "    gradients = trainer.compute_gradients(model.loss)\n",
    "\n",
    "    clipped_gradients = [(tf.clip_by_value(_[0], -conf.grad_clip, conf.grad_clip), _[1]) for _ in gradients]\n",
    "    optimizer = trainer.apply_gradients(clipped_gradients)\n",
    "\n",
    "    saver = tf.train.Saver(tf.trainable_variables())\n",
    "\n",
    "    with tf.Session() as sess: \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        if os.path.exists(conf.ckpt_file):\n",
    "            saver.restore(sess, conf.ckpt_file)\n",
    "            print(\"Model Restored\")\n",
    "       \n",
    "        if conf.epochs > 0:\n",
    "            print(\"Started Model Training...\")\n",
    "        pointer = 0\n",
    "        for i in range(conf.epochs):\n",
    "            for j in range(conf.num_batches):\n",
    "                if conf.data == \"mnist\":\n",
    "                    batch_X, batch_y = data.train.next_batch(conf.batch_size)\n",
    "                    batch_X = binarize(batch_X.reshape([conf.batch_size, \\\n",
    "                            conf.img_height, conf.img_width, conf.channel]))\n",
    "                    batch_y = one_hot(batch_y, conf.num_classes) \n",
    "                else:\n",
    "                    batch_X, pointer = get_batch(data, pointer, conf.batch_size)\n",
    "                data_dict = {X:batch_X}\n",
    "                if conf.conditional is True:\n",
    "                    data_dict[model.h] = batch_y\n",
    "                _, cost = sess.run([optimizer, model.loss], feed_dict=data_dict)\n",
    "            print(\"Epoch: %d, Cost: %f\"%(i, cost))\n",
    "            if (i+1)%10 == 0:\n",
    "                saver.save(sess, conf.ckpt_file)\n",
    "                generate_samples(sess, X, model.h, model.pred, conf, \"\")\n",
    "\n",
    "        generate_samples(sess, X, model.h, model.pred, conf, \"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default='mnist')\n",
    "    parser.add_argument('--layers', type=int, default=12)\n",
    "    parser.add_argument('--f_map', type=int, default=32)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch_size', type=int, default=100)\n",
    "    parser.add_argument('--grad_clip', type=int, default=1)\n",
    "    parser.add_argument('--model', type=str, default='')\n",
    "    parser.add_argument('--data_path', type=str, default='data')\n",
    "    parser.add_argument('--ckpt_path', type=str, default='ckpts')\n",
    "    parser.add_argument('--samples_path', type=str, default='samples')\n",
    "    parser.add_argument('--summary_path', type=str, default='logs')\n",
    "    conf = parser.parse_args()\n",
    "  \n",
    "    if conf.data == 'mnist':\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        if not os.path.exists(conf.data_path):\n",
    "            os.makedirs(conf.data_path)\n",
    "        data = input_data.read_data_sets(conf.data_path)\n",
    "        conf.num_classes = 10\n",
    "        conf.img_height = 28\n",
    "        conf.img_width = 28\n",
    "        conf.channel = 1\n",
    "        conf.num_batches = data.train.num_examples // conf.batch_size\n",
    "    else:\n",
    "        from keras.datasets import cifar10\n",
    "        data = cifar10.load_data()\n",
    "        labels = data[0][1]\n",
    "        data = data[0][0].astype(np.float32)\n",
    "        data[:,0,:,:] -= np.mean(data[:,0,:,:])\n",
    "        data[:,1,:,:] -= np.mean(data[:,1,:,:])\n",
    "        data[:,2,:,:] -= np.mean(data[:,2,:,:])\n",
    "        data = np.transpose(data, (0, 2, 3, 1))\n",
    "        conf.img_height = 32\n",
    "        conf.img_width = 32\n",
    "        conf.channel = 3\n",
    "        conf.num_classes = 10\n",
    "        conf.num_batches = data.shape[0] // conf.batch_size\n",
    "\n",
    "    conf = makepaths(conf) \n",
    "    if conf.model == '':\n",
    "        conf.conditional = False\n",
    "        train(conf, data)\n",
    "    elif conf.model.lower() == 'conditional':\n",
    "        conf.conditional = True\n",
    "        train(conf, data)\n",
    "    elif conf.model.lower() == 'autoencoder':\n",
    "        conf.conditional = True\n",
    "        trainAE(conf, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "tf_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
