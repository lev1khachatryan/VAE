{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>VAE - Probabilistic Models</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of machine learning, we are often interested in learning probabilistic models of various natural and artificial phenomena from data. Probabilistic models are mathematical descriptions of such phenomena. They are useful for understanding such phenomena, for prediction of unknowns in the future, and for various forms of assisted or automated decision making. As such, probabilistic models formalize the notion of knowledge and skill, and are central constructs in the field of machine learning and AI.\n",
    "\n",
    "As probabilistic models contain unknowns and the data rarely paints a complete picture of the unknowns, we typically need to assume some level of uncertainty over aspects of the model. The degree and nature of this uncertainty is specified in terms of (conditional) probability distributions. Models may consist of both continuous-valued variables and discrete-valued variables. The, in some sense, most complete forms of probabilistic models specify all correlations and higher-order dependencies between the variables in the model, in the form of a joint probability distribution over those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use $x$ as the vector representing the set of all observed variables whose joint distribution we would like to model. Note that for notational simplicity and to avoid clutter, we use lower case bold to denote the underlying set of observed random variables, i.e. flattened and concatenated such that the set is represented as a single vector.\n",
    "\n",
    "We assume the observed variable $x$ is a random sample from an ***unknown underlying process***, whose true (probability) distribution $p^*(x)$ is unknown. We attempt to approximate this underlying process with a chosen model $p_{\\theta}(x)$, with parameters $\\theta$: $x ∼ p_{\\theta}(x)$\n",
    "\n",
    "Learning is, most commonly, the process of searching for a value of the parameters $\\theta$ such that the probability distribution function given by the model, $p_{\\theta}(x)$, approximates the true distribution of the data, denoted by $p^*(x)$, such that for any observed $x$: $p_{\\theta}(x) \\approx p^*(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, such as in case of classification or regression problems, we are not interested in learning an unconditional model $p_{\\theta}(x)$, but a conditional model $p_{\\theta}(y|x)$ that approximates the underlying conditional distribution $p^*(y|x)$: a distribution over the values of variable $y$, conditioned on the value of an observed variable x. In this case, $x$ is often called the input of the model. Like in the unconditional case, a model $p_{\\theta}(y|x)$ is chosen, and optimized to be close to the unknown underlying distribution, such that for any $x$ and $y$: $p_{\\theta}(y|x) \\approx p^*(y|x)$.\n",
    "\n",
    "A relatively common and simple example of conditional modeling is image classification, where $x$ is an image, and $y$ is the image’s class, as labeled by a human, which we wish to predict.\n",
    "\n",
    "Conditional models become more difficult to learn when the predicted variables are very ***high-dimensional***, such as images, video or sound. One example is the reverse of the image classification problem: prediction of a distribution over images, conditioned on the class label. Another example with both high-dimensional input, and highdimensional output, is time series prediction, such as text or video prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of neural network based image classification, neural networks parameterize a categorical distribution $p_{\\theta}(y|x)$ over a class label $y$, conditioned on an image $x$:\n",
    "\n",
    "<center>$p = NeuralNet(x)$</center>\n",
    "<center>$p_{\\theta}(y|x) = Categorical(y; p)$</center>\n",
    "\n",
    "where the last operation of $NeuralNet(.)$ is typically a $softmax()$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
