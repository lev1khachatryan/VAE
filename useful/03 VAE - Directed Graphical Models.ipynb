{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>VAE - Directed Graphical Models</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work with ***directed probabilistic models***, also called ***directed probabilistic graphical models (PGMs)***, or ***Bayesian networks***. Directed graphical models are a type of probabilistic models where all the variables are topologically organized into a directed acyclic graph. The joint distribution over the variables of such models factorizes as a product of prior and conditional distributions:\n",
    "\n",
    "<center>$p_{\\theta}(x_1, ..., x_M) = \\prod_{j=1}^{M} p_{\\theta}(x_j | Pa(x_j)) $</center>\n",
    "\n",
    "where $Pa(x_j)$ is the set of parent variables of node $j$ in the directed graph. For non-root-nodes, we condition on the parents. For root nodes, the set of parents is the empty set, such that the distribution is unconditional.\n",
    "\n",
    "To parameterize a conditional probability distribution $p_{\\theta}(x_j | Pa(x_j))$ we can use neural networks. In this case, neural networks take as input the parents of a variable in a directed graph, and produce the distributional parameters $\\eta$ over that variable:\n",
    "\n",
    "<center>$ \\eta = NeuralNet(Pa(x)) $</center>\n",
    "<center>$ p_{\\theta}(x | Pa(x)) = p_{\\theta}(x | \\eta )$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often collect a dataset $D$ consisting of $N >= 1$ datapoints:\n",
    "\n",
    "<center>$ D = \\{ x^1, x^2, ..., x^N \\} = \\{ x^i \\}_{i=1}^N = x^{(1:N)} $</center>\n",
    "\n",
    "The datapoints are assumed to be independent samples from an unchanging underlying distribution. In other words, the dataset is assumed to consist of distinct, independent measurements from the same (unchanging) system. In this case, the observations $D$ are said to be i.i.d., for independently and identically distributed. Under the i.i.d. assumption, the probability of the datapoints given the parameters factorizes as a product of individual datapoint probabilities. The log-probability assigned to the data by the model is therefore given by:\n",
    "\n",
    "<center>$ log p_{\\theta} (D) = \\sum_{x \\in D} log p_{\\theta} (x) $</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common criterion for probabilistic models is ***maximum loglikelihood (ML)***. As we will explain, maximization of the log-likelihood criterion is equivalent to minimization of a ***Kullback Leibler divergence*** between the data and model distributions. Under the ML criterion, we attempt to find the parameters $\\theta$ that maximize the sum, or equivalently the average, of the log-probabilities assigned to the data by the model. With i.i.d. dataset $D$ of size $N_D$, the maximum likelihood objective is to maximize the log-probability given by the above equation.\n",
    "\n",
    "Using calculusâ€™ chain rule and automatic differentiation tools, we can efficiently compute gradients of this objective, i.e. the first derivatives of the objective w.r.t. its parameters $\\theta$. We can use such gradients to iteratively hill-climb to a local optimum of the ML objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
